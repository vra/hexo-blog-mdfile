---
title: 关于LLaVA-Plus 的一些思考
date: 2023-11-12 12:57:30
tags:
 - AI 
 - LLM
 - LMM
 - CV
 - Deep Learning
---

[LLaVA-Plus](https://llava-vl.github.io/llava-plus) 是LLAVA团队最近放出来的LMM工作，对LLaVA进行了改进升级，相比LLaVA对输入图像只能进行文本回答的情况，LLaVA-PLUS则包含相当丰富的功能：
+ 可以调用SD生成与输入类似的图像
+ 可以对图像进行编辑，例如调用Instruct pix2pix在图像上放置一只动物
+ 可以对图像进行物体检测，分割，Cpation，OCR，打标签等多模态处理的功能
+ 还可以调用外部知识来对未知的信息进行检索
+ 支持用户交互，如对用户点击的区域进行实例分割
+ 对图像进行美化，然后生成可以发布到社交媒体上的文案

那么LMM是怎么获得到这么多的多模态能力的呢？论文中提出了一个叫`Skill Repository` 的概念，就是一些AI 子任务的能力和对应的模型，利用这个Skill Repository来完成丰富的功能。也就是说LLaVA-Plus将用户输入的任务通过进行拆分，然后调用合适的子任务模型来实现，再对结果进行一定的处理返回给用户。
<!--more-->

具体的Skill Repository 包括下面这些：
![](/imgs/llava_plus/20231112094107.png)

其实会发现，这种思路跟Visual ChatGPT是很类似的，不过与Visual ChatGPT不同的是，LLaVA-Plus没有调用外部的大语言模型ChatGPT，而是将LLM部分融合进了统一的网络结构中。这样的好处是图像特征在整个对话过程中都是有感知的，而Visual ChatGPT的方案则只在调用子模型的时候有图像感知，语言模型部分并不知道图像的特征，毕竟那会的ChatGPT还无法理解图像。

我觉得这种思路是LMM模型最可行的方案，即语言模型部分理解用户的要求，得到需要调用能力的列表，再调用对应的多模态模型，将多模态模型的输出进行总结，以自然语言的形式返回给用户。
这样的好处也是非常明显的：
+ 将子任务模型与LMM模型解耦，只要增加自己子任务的模型，就能支持用户的输入要求
+ 每个子任务模型解决自己的特定的任务，结果肯定是最好的，而不是用一个什么都想做但都做的不是最好的模型
+ 子任务可以利用现有的开源模型，降低整个系统学习的难度，避免了重复工作


我觉得LLaVA-Plus对AI应用的进一步涌现很有促进作用。首先是这个方向有很多有意思的东西可以来做着玩了。
比如自动发朋友圈/微博/Ins/Twitter的Bot，可以将用户拍的照片进行美化，提高分辨率，然后自动生成I文案并发送出去。更发散一点，AI可以有自己的朋友圈了。

还有自动标注数据集的工具，所有类型的标注都自动来标注，甚至可以利用不同模型之间的一致性对标注质量进行提高。

另一方面，包含语言模型和子任务模型的LMMs也许真的会让CV和AI离普通人更近，因为自然语言的接口相比之前的计算机语言的接口要更易用。也许未来我们真的不需要单独的子任务模型了，通过LLaVA-Plus就可以用自然语言调用这些模型，甚至未来这些子任务模型我们可能都感知不到了，毕竟对用户来说，只是希望解决问题，而不关系底层用的是检测模型还是分割模型。

![](/imgs/llava_plus/20231112093744.png)


![](/imgs/llava_plus/20231112095143.png)![](/imgs/llava_plus/20231112095628.png)

另一个有意思的结果是，利用LLaVA-Plus可以对文生图的过程进行改进，就像WALLE-3利用ChatGPT来生成更好的Prompt一样，LLaVA-Plus也可以对用户输入的提示词进行优化，得到更适合SD的提示词：
![](/imgs/llava_plus/20231112132234.png)

最后对论文的大致思路进行一个总结，也是比较粗糙，具体细节看论文吧。
作者提出了一种通用的多模态任务的问答形式：
![](/imgs/llava_plus/20231112122754.png)

Iq是问题图像输入，Xq是问题文本输入，Xanswer是回答输出。

形式看似简单，但要看到这种统一形式的重要意义，利用统一的形式定义，可以将大量的不同子任务训练数据组织到一起，为LMM强大功能奠定基础。

为了得到准确的Xanswer，需要借助 Skill Repository里面的工具，得到Xskill_result，再得到Xanswer，![](/imgs/llava_plus/20231112094913.png)
![](/imgs/llava_plus/20231112122933.png)
为了能够找到输入任务对应的模型并得到输出，作者设置了"thoughts", "actions"和“value"三个阶段的， 分别进行输入到子模型的拆分、子模型调用API和参数，以及子模型的输出。

下面是一个具体调用的例子：
![](/imgs/llava_plus/20231112123129.png)

在训练数据的构造方面也比较有意思。
为了利用LLaVA没有thoughts-actions-value过程的数据，作者添加了“空白”的thoughts-actions-value占位符：
![](/imgs/llava_plus/20231112131254.png)

为了增加问题的多样性，让GPT4来改写问题：![](/imgs/llava_plus/20231112131711.png)

根据caption 数据，让ChatGPT/GPT4来提问题，构造训练数据，这里的提示词工程挺有意思，有些trick在里面，可以细看一下：
![](/imgs/llava_plus/20231112131934.png)

论文附录中有很多例子，可以参考。

Online功能体验地址： [LLaVA-Plus (llavaplus.ngrok.io)](https://llavaplus.ngrok.io/)

