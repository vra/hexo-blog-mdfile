---
title: Pytorch Apple Silicon GPU è®­ç»ƒä¸æµ‹è¯„
date: 2022-05-19 16:46:24
tags:
- Mac
- Pytorch
- Python
- GPU
---
ä»Šå¤©ä¸­åˆçœ‹åˆ°Pytorchçš„å®˜æ–¹åšå®¢å‘äº†Apple M1 èŠ¯ç‰‡ GPUåŠ é€Ÿçš„æ–‡ç« ï¼Œè¿™æ˜¯æˆ‘æœŸå¾…äº†å¾ˆä¹…çš„åŠŸèƒ½ï¼Œå› æ­¤å¾ˆå…´å¥‹ï¼Œç«‹é©¬è¿›è¡Œæµ‹è¯•ï¼Œç»“è®ºæ˜¯åœ¨MNISTä¸Šï¼Œé€Ÿåº¦ä¸P100å·®ä¸å¤šï¼Œç›¸æ¯”CPUæé€Ÿ1.7å€ã€‚å½“ç„¶è¿™åªæ˜¯ä¸€ä¸ªæœ€ç®€å•çš„ä¾‹å­ï¼Œä¸èƒ½åæ˜ å¤§éƒ¨åˆ†æƒ…å†µã€‚è¿™é‡Œè¯¦ç»†è®°å½•æ“ä½œçš„ä¸€æ­¥æ­¥æµç¨‹ï¼Œå¦‚æœä½ ä¹Ÿæ„Ÿå…´è¶£ï¼Œä¸å¦¨è‡ªå·±ä¸Šæ‰‹ä¸€è¯•ã€‚
<!--more-->

## åŠ é€ŸåŸç†
è‹¹æœæœ‰è‡ªå·±çš„ä¸€å¥—GPUå®ç°API Metalï¼Œè€ŒPytorchæ­¤æ¬¡çš„åŠ é€Ÿå°±æ˜¯åŸºäºMetalï¼Œå…·ä½“æ¥è¯´ï¼Œä½¿ç”¨è‹¹æœçš„Metal Performance Shadersï¼ˆMPSï¼‰ä½œä¸ºPyTorchçš„åç«¯ï¼Œå¯ä»¥å®ç°åŠ é€ŸGPUè®­ç»ƒã€‚MPSåç«¯æ‰©å±•äº†PyTorchæ¡†æ¶ï¼Œæä¾›äº†åœ¨Macä¸Šè®¾ç½®å’Œè¿è¡Œæ“ä½œçš„è„šæœ¬å’ŒåŠŸèƒ½ã€‚MPSé€šè¿‡é’ˆå¯¹æ¯ä¸ªMetal GPUç³»åˆ—çš„ç‹¬ç‰¹ç‰¹æ€§è¿›è¡Œå¾®è°ƒçš„å†…æ ¸æ¥ä¼˜åŒ–è®¡ç®—æ€§èƒ½ã€‚æ–°è®¾å¤‡åœ¨MPSå›¾å½¢æ¡†æ¶å’ŒMPSæä¾›çš„è°ƒæ•´å†…æ ¸ä¸Šæ˜ å°„æœºå™¨å­¦ä¹ è®¡ç®—å›¾å½¢å’ŒåŸºå…ƒã€‚

å› æ­¤æ­¤æ¬¡æ–°å¢çš„çš„deviceåå­—æ˜¯`mps`ï¼Œä½¿ç”¨æ–¹å¼ä¸`cuda`ç±»ä¼¼ï¼Œä¾‹å¦‚ï¼š
```python
import torch
foo = torch.rand(1, 3, 224, 224).to('mps')

device = torch.device('mps')
foo = foo.to(device)
```

æ˜¯ä¸æ˜¯ç†Ÿæ‚‰çš„é…æ–¹ï¼Œç†Ÿæ‚‰çš„å‘³é“ï¼Ÿå¯ä»¥è¯´æ˜¯æ— é—¨æ§›å³å¯ä¸Šæ‰‹ã€‚


æ­¤å¤–å‘ç°ï¼ŒPytorchå·²ç»æ”¯æŒä¸‹é¢è¿™äº›deviceäº†ï¼Œç¡®å®å‡ºä¹æ„æ–™:
+ cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, ort, mps, xla, lazy, vulkan, meta, hpu


## ç¯å¢ƒé…ç½®
ä¸ºäº†ä½¿ç”¨è¿™ä¸ªå®éªŒç‰¹æ€§ï¼Œä½ éœ€è¦æ»¡è¶³ä¸‹é¢ä¸‰ä¸ªæ¡ä»¶ï¼š
1. æœ‰ä¸€å°é…æœ‰Apple Silicon ç³»åˆ—èŠ¯ç‰‡ï¼ˆM1, M1 Pro, M1 Pro Max, M1 Ultra)çš„Macç¬”è®°æœ¬
2. å®‰è£…äº†**arm64**ä½çš„Python
3. å®‰è£…äº†æœ€æ–°çš„`nightly`ç‰ˆæœ¬çš„Pytorch 

ç¬¬ä¸€ä¸ªæ¡ä»¶éœ€è¦ä½ è‡ªå·±æ¥è®¾æ³•æ»¡è¶³ï¼Œè¿™ç¯‡æ–‡ç« å¯¹å®ƒçš„è¾¾åˆ°æ²¡æœ‰ä»€ä¹ˆå¸®åŠ©ã€‚

å‡è®¾æœºå™¨å·²ç»å‡†å¤‡å¥½ã€‚æˆ‘ä»¬å¯ä»¥ä»[è¿™é‡Œ](https://docs.conda.io/en/latest/miniconda.html)ä¸‹è½½arm64ç‰ˆæœ¬çš„miniconda(æ–‡ä»¶åæ˜¯`Miniconda3 macOS Apple M1 64-bit bash`)ï¼ŒåŸºäºå®ƒå®‰è£…çš„Pythonç¯å¢ƒå°±æ˜¯arm64ä½çš„ã€‚ä¸‹è½½å’Œå®‰è£…Minicodaçš„å‘½ä»¤å¦‚ä¸‹ï¼š
```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh 
chmod +x Miniconda3-latest-MacOSX-arm64.sh 
./Miniconda3-latest-MacOSX-arm64.sh 
```
æŒ‰ç…§è¯´æ˜æ¥æ“ä½œå³å¯ï¼Œå®‰è£…å®Œæˆåï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼Œé€šè¿‡æ£€æŸ¥`platform.uname()[4]`æ˜¯ä¸æ˜¯ä¸º`arm64`æ¥æ£€æŸ¥Pythonçš„æ¶æ„:
```bash
conda config --env --set always_yes true
conda create -n try-mps python=3.8
conda activate try-mps
python -c "import platform; print(platform.uname()[4])"
```
å¦‚æœæœ€åä¸€å¥å‘½ä»¤çš„è¾“å‡ºä¸º`arm64`ï¼Œè¯´æ˜Pythonç‰ˆæœ¬OKï¼Œå¯ä»¥ç»§ç»­å¾€ä¸‹èµ°äº†ã€‚  

ç¬¬ä¸‰æ­¥ï¼Œå®‰è£…nightlyç‰ˆæœ¬çš„Pytorchï¼Œåœ¨å¼€å¯çš„è™šæ‹Ÿç¯å¢ƒä¸­è¿›è¡Œä¸‹é¢çš„æ“ä½œï¼š
```bash
python -m pip  install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu
```
æ‰§è¡Œå®Œæˆåé€šè¿‡ä¸‹é¢çš„å‘½ä»¤æ£€æŸ¥MPSåç«¯æ˜¯å¦å¯ç”¨:
```bash
python -c "import torch;print(torch.backends.mps.is_built())"
```
å¦‚æœè¾“å‡ºä¸º`True`ï¼Œè¯´æ˜MPSåç«¯å¯ç”¨ï¼Œå¯ä»¥ç»§ç»­å¾€ä¸‹èµ°äº†ã€‚


## è·‘ä¸€ä¸ªMNIST
åŸºäºPytorchå®˜æ–¹çš„exampleä¸­çš„MNISTä¾‹å­ï¼Œä¿®æ”¹äº†æ¥æµ‹è¯•cpuå’Œmpsæ¨¡å¼ï¼Œä»£ç å¦‚ä¸‹:
```python mark:85
from __future__ import print_function
import argparse
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output


def train(args, model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
            if args.dry_run:
                break


def main():
    # Training settings
    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
    parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                        help='input batch size for training (default: 64)')
    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=4, metavar='N',
                        help='number of epochs to train (default: 14)')
    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',
                        help='learning rate (default: 1.0)')
    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
                        help='Learning rate step gamma (default: 0.7)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--use_gpu', action='store_true', default=False,
                        help='enable MPS')
    parser.add_argument('--dry-run', action='store_true', default=False,
                        help='quickly check a single pass')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                        help='how many batches to wait before logging training status')
    parser.add_argument('--save-model', action='store_true', default=False,
                        help='For Saving the current Model')
    args = parser.parse_args()
    use_gpu = args.use_gpu

    torch.manual_seed(args.seed)

    device = torch.device("mps" if args.use_gpu else "cpu")

    train_kwargs = {'batch_size': args.batch_size}
    test_kwargs = {'batch_size': args.test_batch_size}
    if use_gpu:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True}
        train_kwargs.update(cuda_kwargs)
        test_kwargs.update(cuda_kwargs)

    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
        ])
    dataset1 = datasets.MNIST('../data', train=True, download=True,
                       transform=transform)
    dataset2 = datasets.MNIST('../data', train=False,
                       transform=transform)
    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)

    model = Net().to(device)
    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
    for epoch in range(1, args.epochs + 1):
        train(args, model, device, train_loader, optimizer, epoch)
        test(model, device, test_loader)
        scheduler.step()


if __name__ == '__main__':
    t0 = time.time()
    main()
    t1 = time.time()
    print('time_cost:', t1 - t0)
```
æµ‹è¯•CPUï¼š
```bash
python main.py
```

æµ‹è¯•MPS:
```bash
python main --use_gpu
```
åœ¨M1æœºå™¨ä¸Šæµ‹è¯•å‘ç°ï¼Œè®­ä¸€ä¸ªEpochçš„MNISTï¼ŒCPUè€—æ—¶33.4sï¼Œè€Œä½¿ç”¨MPSçš„è¯è€—æ—¶19.6sï¼ŒåŠ é€Ÿ1.7å€ï¼Œå¥½åƒæ²¡å®˜æ–¹åšå®¢ä¸­è¯´çš„é‚£ä¹ˆå¤šï¼Œä¼°è®¡æ˜¯è·Ÿæ¨¡å‹å¤ªå°æœ‰å…³ã€‚

æˆ‘åˆåœ¨Nvidia P100 GPUæœåŠ¡å™¨ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒCPUè€—æ—¶34.2sï¼Œä½¿ç”¨CUDA è€—æ—¶20.4sï¼ŒåŠ é€Ÿæ¯”1.67å€ï¼Œè·ŸM1å·®ä¸å¤šï¼Œæ•´ä½“é€Ÿåº¦ç•¥ä½äºM1ã€‚
ä¸‹é¢æ˜¯ä¸€ä¸ªæ€»ç»“è¡¨æ ¼ï¼š

|æœºå™¨|å†…å­˜|CPUè€—æ—¶|GPUè€—æ—¶|åŠ é€Ÿæ¯”|
|-|-|--|-|-|
|M1|16G|33.4s|19.6s|1.70|
|P100|256G|34.2s|20.4s|1.67|

## è·‘ä¸€ä¸‹VAEæ¨¡å‹
ç±»ä¼¼åœ°ï¼Œè·‘ä¸€ä¸‹è¿™ä¸ªä»“åº“é‡Œé¢åœ°VAEæ¨¡å‹ï¼Œå‘ç°CPUæ¨¡å¼æ­£å¸¸ï¼Œæ¢æˆMPSålossä¸æ–­å¢å¤§ï¼Œæœ€ååˆ°nanï¼Œçœ‹æ¥è¿˜æ˜¯æœ‰bugçš„ (æ¯•ç«Ÿæ˜¯å®éªŒç‰¹æ€§)ï¼Œå¯ä»¥åœ¨Pytorch GitHub ä»“åº“é‡Œé¢æissueï¼ŒæœŸå¾…æ›´å¥½çš„Pytorchã€‚
```bash
[W ParallelNative.cpp:229] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)
Train Epoch: 1 [0/60000 (0%)]   Loss: 550.842529
Train Epoch: 1 [1280/60000 (2%)]        Loss: 330.613251
Train Epoch: 1 [2560/60000 (4%)]        Loss: 4705.016602
Train Epoch: 1 [3840/60000 (6%)]        Loss: 183532752.000000
...
Train Epoch: 6 [40960/60000 (68%)]      Loss: nan
Train Epoch: 6 [42240/60000 (70%)]      Loss: nan
```

## ä¸€ä¸ªæ„¿æ™¯
å¼€å¤´æåˆ°ï¼Œå…³æ³¨è¿™ä¸ªç‰¹æ€§æŒºä¹…äº†ï¼Œå…¶å®æˆ‘æœ€åˆçš„æƒ³æ³•ï¼Œæ˜¯å¸Œæœ›ä¸€å°æ™®é€šè®¡ç®—è®¾å¤‡ï¼ˆä¸å¸¦GPUçš„ç¬”è®°æœ¬ï¼Œæ™ºèƒ½æ‰‹æœºï¼‰éƒ½èƒ½è®­éå¸¸å¿«çš„æ¨¡å‹ã€‚å› ä¸ºGPUå¡å¾ˆæ˜‚è´µï¼Œåªæœ‰ç§‘ç ”æœºæ„å’Œå¤§å…¬å¸æ‰æœ‰ï¼Œæ™®é€šäººè´­ä¹°æˆæœ¬æ¯”è¾ƒé«˜ï¼Œè€Œäº‘æœåŠ¡å•†æä¾›çš„GPUæŒ‰æ—¶æ”¶è´¹ï¼Œä»·æ ¼ä¸è²ã€‚å¦ä¸€æ–¹é¢ï¼Œæ‰€æœ‰æ™®é€šç¬”è®°æœ¬å’Œæ™ºèƒ½æ‰‹æœºéƒ½æœ‰ä¸é”™çš„CPUï¼Œç®—åŠ›ä¸é”™ï¼Œå¦‚æœèƒ½å°†è¿™éƒ¨åˆ†æ€§èƒ½åˆç†åœ°åˆ©ç”¨èµ·æ¥ï¼Œå°±åƒæ·±åº¦å­¦ä¹ å‰çš„æ—¶ä»£ä¸€æ ·ï¼Œæœ‰ä¸€å°ç¬”è®°æœ¬å°±èƒ½ç”¨MatLabå¿«é€Ÿåœ°è¿›è¡Œç§‘å­¦å®éªŒï¼Œè¿™æ ·æ‰èƒ½å°†AIæ¨å¹¿åˆ°æ›´å¤šäººï¼Œå°†AIå¹³æ°‘åŒ–ï¼Œä¹Ÿé¿å…äº†å¤§å…¬å¸åœ¨ç¡¬ä»¶èµ„æºä¸Šçš„å„æ–­å’Œæ˜¾å¡å·¨å¤§çš„èƒ½è€—ã€‚

ä»Šå¤©çš„Mac GPUè®­ç»ƒè‡³å°‘æ˜¯åœ¨é™ä½æ·±åº¦å­¦ä¹ èƒ½è€—å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„"è½»é‡åŒ–"ä¸Šé¢æœ‰äº†ä¸€ä¸ªå¤§çš„è¿›æ­¥ï¼Œä½ å¯ä»¥æŠ±ç€ç¬”è®°æœ¬åœ¨åºŠä¸Šè®­ç»ƒæ”¹å˜AIæ¨¡å‹äº† ğŸ˜Š ã€‚ä½†ä»¥Macç¬”è®°çš„ä»·æ ¼ï¼Œå¾ˆéš¾è¯´åœ¨å¹³æ°‘åŒ–æ–¹å‘ä¸Šæœ‰ä»»ä½•çš„è¿›å±•ã€‚
