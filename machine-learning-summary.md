title: 机器学习笔记-总结
date: 2015-12-12 13:43:42
tags:
- 机器学习
- 总结
---
机器学习笔记是我这学期在上"统计学习"这门课时学习到的内容的一个总结.因为过往很多学过的知识,现在大多都已经忘掉了,而统计机器学习的内容则很重要,我可不能再上过就忘掉,所以在复习的时候把这些内容都记录下来,以便以后查阅.
<!--more-->

## 基本概念
 1. 学习：一个系统在完成一项任务T的时候,使用了数据E,使得在评判标准P下,性能获得了提升,这就是学习  
 2. 统计学习的对象是数据,关于统计学习的基本假设是：同类数据服从一定的统计规律性,即数据都是独立同分布的  
 3. 统计学习中的问题可以分为4大类,分别是 
 	- 监督学习：解决**预测**问题, 根据预测变量是否连续,分为回归问题和分类问题
 	- 非监督学习：解决**分析**型问题, 分为聚类(.如图像分割).,密度估计和关联分析三类
 	- 半监督学习：问题中既有分析的部分,又有预测的部分,主要有主动学习(.先分析,出现问题时向人要数据(.如分类label).).
 	- 增强学习：在合适过程中,根据反馈做新的判断,主动增强自身的学习,典型应用：机器人足球
 4. 统计学习的基本步骤
  1. 获取数据(.即E).
  2. 确定用什么样的数学模型,所有模型构成假设空间
  3. 有了一组模型后,确定策略,即如何来找到最优的模型
  4. 写出模型选择策略的算法
  5. 通过学习得到最优模型
  6. 用学习到的模型来在新的数据上进行分析和预测
 5. 学习的三大要素：模型,策略,算法.模型是知识的集合,策略是模型选择的准则,算法就是学习的具体方法

## 模型的种类
 1. 线性模型(Linear model)：y=ax+b,算法实例：线性回归(Linear regression)
 2. 对数线性(Log-linear model)：算法实例：逻辑斯蒂回归(Logistic regression)
 3. 稀疏模型(Sparse model)：形式也是y=ax+b,但a很稀疏,算法实例：稀疏分解(Sparese decomposition)
 4. 非线性核方法(Non-linear by kernel),利用核技巧将非线性问题转化为线性问题,实例：支持向量机(SVM, support vector machine)
 5. 层级非线性(Layered nonlinear)：实例：神经网络(Neural Network)
 6. 图模型(Graphic model)：将数据看作随机变量的话,它们之间的依赖关系可用图来表示,实例：贝叶斯网络(Bayes network)
 7. 树模型(Tree model)：对输入变量进行分块处理,每个子块有可以使用别的机器学习算法,实例：决策树(Decision tree),提升数(Boosting tree)
 8. 混合模型(Mixture model)：实例：聚类(Clustering)
上述模型中,1-5为非概率模型,6,8为概率模型,7为混合类型,概率模型和非概率模型可能都有.

## 学习的策略
策略是模型选择的准则,为了量化模型的好坏,我们定义了损失函数和风险函数  
损失函数(Loss function)：也叫代价函数(Cost function),用来度量模型对于**一个**输入`X`产生的预测值`f(x)`与真实值`Y`之间的差异的大小.常见的损失函数有:
 1. 0-1 损失函数(0-1 loss function)
![](/imgs/01loss.png)
 2. 平方损失函数(Square loss function)
![](/imgs/pingfang.png)
 3. 绝对值损失函数(Absolute loss function)
![](/imgs/jueduizhi.png)
 4. 对数损失函数(Log loss function)
![](/imgs/duishu.png)
风险函数是损失函数的期望,即将模型的输入输出`XY`作为随机变量,风险函数就是模型`f(X)`关于联合分布`P(X,Y)`的平均意义下的损失.风险函数的值越小,表示模型预测结果越准确,这种模型就越好,所以机器学习的目的就是最小化风险函数(Rish miniization).需要注意的是：`P(X,Y)`是未知的  
如果给定数据集,我们可以计算在该数据集上的平均损失,这个损失定义为经验风险.经验风险在数据量足够大的时候,能很好的近似期望风险,但在数据量较少的时候误差会比较大.
在经验风险的基础上,加上表示模型复杂度的正则化项,则构成结构风险.结构风险能有效的防止过拟合,因为结构风险要求经验风险和模型复杂度同时都小.  

## 经典机器学习算法
分类算法：
 1. K近邻(KNN, K Nearest Neighbor)
 2. 朴素贝叶斯(Naive Bayes)
 3. 支持向量机(SVM, Support Vector Machine)
 4. AdaBoost 

聚类算法：
 1. K-Means 
 2. 期望最大化(EM, Expectation Maximization)

回归算法：
 1. 脊回归(Ridge regression)
 2. Lasso回归(The Least Absolute Shrinkage and Selectionator Operator)

关联分析算法：
 1. 先验算法(Aprior)

降维算法：
 1. 主成份分析法(PCA, Principal Component Analysis)
 2. 局部线性嵌入(Locally linear embedding)

## 欠拟合(under-fitting)和过拟合(over-ftting)
在训练模型的时候,有的时候需要选择不同的复杂度(.如不同参数的个数).来训练,不同的复杂度体现了模型对训练数据的拟合程度.   
如果参数过少,模型过于简单,则模型不能很好的拟合训练数据,这种情况称为欠拟合,很显然,欠拟合因为连训练数据的规律都没有学习到,所以对于预测,性能肯定不会太好.  
另一方面,如果参数太多,模型过于复杂,则对训练数据可以做到特别好的拟合,但由于训练数据是有噪声和误差的,这种情况会将训练数据的噪声和误差都考虑进来,在测试集上性能反而会下降.下面是训练误差和测试误差与模型复杂度的关系
![](/imgs/overfitting.gif)

## 交叉验证(Cross Validation)
学习的最终目的是预测,即学习一个模型,使得对未知数据能很好地预测.在实际操作中,一般将数据集分为3部分：训练集,验证集和测试集.为了验证在训练集上学习到的模型好坏,需要现在验证集上进行验证.实际中数据总是不充足的,所以需要重复使用数据,采用交叉验证的方法.最常用的交叉验证方法是S折交叉验证方法.  
S折交叉验证方法(S-fold cross validation)：随机地将数据切分为S个互不相交的子集,然后利用S-1个子集的数据训练模型,利用余下的1个子集作为测试集.测试集的选择有S中情况,所以这种验证可以进行S次.对每个模型,都进行S次训练和验证,然后求出平均测试误差,将平均测试误差最小的模型作为最优模型.  
当数据量特别少的时候,我们将每个数据分为一个子集,即如果有N个数据,则S=N,这种方法称为留一交叉验证(Leave-one-out cross validation).

## 判别模型(Discriminative model)和生成模型(generative model)
生成式方法：对于某个给定的输入`X`,先学习得到联合分布`P(X,Y)`,再计算`P(Y|X)`,也即该方法考虑给定输入`X`,输出`Y`是怎么生成的,要求得到一个关于整体的信息,即对`P(X,Y)`进行建模.   
生成式方法应用更广,适用于各种机器学习问题,而且收敛速度快,而且对于有隐变量的情况,也适用.但由于需要建模`XY`的联合分布,所以不能进行降维处理. 
常见的生成式模型有朴素贝叶斯法和隐马尔科夫模型.  
判别式方法：对于某个给定的输入`X`,直接给出预测值`f(X)`或`P(Y|X)`.该方法关注的是对于给定的输入`X`,应该预测什么样的输出`Y`,而不用去考虑数据整体的分布这些信息,即对`P(Y|X)`建模.   
常见的判别模型有KNN,感知机,决策树,逻辑斯蒂回归,最大熵模型,SVM,AdaBoost,条件随机场等.  
判别式方法只能用于分类和回归问题,可以对`X`进行降维处理.  

